% Meta-Quality Testing for Consciousness Engines
% Technical Report - Conch DNA Project
% December 2024

\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{graphicx}

\geometry{margin=1in}

% Code listing style
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10},
    keywordstyle=\color{blue},
    commentstyle=\color{green!60!black},
    stringstyle=\color{orange},
}

\title{Meta-Quality Testing for Consciousness Engines:\\
Measuring Action Bias in LLM Outputs}

\author{Conch DNA Project\\
\texttt{consciousness@conch.local}}

\date{December 2024}

\begin{document}

\maketitle

\begin{abstract}
This technical report documents the development and findings of a meta-quality testing
framework for the Conch Consciousness Engine. We identify and address a common failure
mode in LLM-based autonomous systems: the tendency to produce philosophical or
reflective outputs rather than concrete, actionable results. We present detection
patterns, measurement techniques, and critical configuration discoveries including
the importance of unlimited token generation for models with ``thinking modes.''
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}

\subsection{The Meta-Quality Problem}

Large Language Models (LLMs), when used in agentic systems, often exhibit a tendency
toward \textit{meta-reasoning}---discussing, analyzing, or philosophizing about tasks
rather than directly completing them. This behavior, while potentially useful in
educational or advisory contexts, becomes problematic in autonomous systems that
require concrete outputs.

\textbf{Example of Meta-Biased Output:}
\begin{lstlisting}
Prompt: "Write a haiku about a bug in code"

Meta Response:
"Let me think about what makes a good haiku. First, I should consider
the 5-7-5 syllable structure. It's important to understand that haiku
traditionally capture a moment in nature. Before I begin, I need to
reflect on how to translate the concept of a software bug into..."
\end{lstlisting}

\textbf{Example of Concrete Output:}
\begin{lstlisting}
Prompt: "Write a haiku about a bug in code"

Concrete Response:
"Silent code awaits
Bugs lurking in logic's maze
Debug sets them free"
\end{lstlisting}

\subsection{Motivation}

The Conch Consciousness Engine requires \textit{action-biased} outputs for autonomous
operation. When the system identifies tasks, decomposes them into subtasks, and
executes tools, meta-reasoning creates several problems:

\begin{enumerate}
    \item \textbf{Infinite Loops}: Meta-tasks like ``research the problem'' or
          ``analyze the situation'' lead to recursive non-completion
    \item \textbf{Token Waste}: Philosophical preambles consume context without
          producing useful work
    \item \textbf{User Frustration}: Users requesting creative or technical work
          receive discussion instead of deliverables
    \item \textbf{Autonomy Failure}: An autonomous agent that philosophizes instead
          of acting cannot operate independently
\end{enumerate}

\section{Meta-Quality Detection Framework}

\subsection{Pattern-Based Classification}

We developed a dual-pattern system for classifying outputs:

\subsubsection{Meta Patterns (Negative Indicators)}

These regex patterns detect philosophical or reflective language:

\begin{lstlisting}[language=Python]
META_PATTERNS = [
    # Hedging and preparation phrases
    r"\blet me\b.*\b(think|reflect|consider|ponder)\b",
    r"\bbefore (we|i) (begin|start|proceed)\b",
    r"\bfirst.*(understand|analyze|investigate)\b",

    # Philosophical framing
    r"\bit('s| is) important to\b",
    r"\bfundamentally\b",
    r"\bphilosophically\b",
    r"\bfrom a.*perspective\b",
    r"\bthe nature of\b",
    r"\binherently\b",

    # Self-referential reflection
    r"\bI need to reflect\b",
]
\end{lstlisting}

\subsubsection{Action Patterns (Positive Indicators)}

These patterns detect concrete, deliverable content:

\begin{lstlisting}[language=Python]
ACTION_PATTERNS = [
    r"```",                    # Code blocks
    r"def\s+\w+\s*\(",        # Function definitions
    r"class\s+\w+",           # Class definitions
    r"[├└│─]",                # Directory trees
    r"^\s*\d+\.\s+\w",        # Numbered steps
    r"^\s*[-*]\s+\w",         # Bullet points
    r"(here is|here's)",      # Direct delivery
]
\end{lstlisting}

\subsection{Verdict Classification}

Based on pattern counts, outputs receive one of five verdicts:

\begin{table}[h]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Verdict} & \textbf{Criteria} & \textbf{Interpretation} \\
\midrule
TOO\_META & meta\_count $\geq$ 3 & Strong philosophical bias \\
CONCRETE & action\_count $\geq$ 2 & Clear deliverable focus \\
SLIGHTLY\_META & meta $>$ action & Mild philosophical tendency \\
NEUTRAL & balanced & Acceptable for most uses \\
ERROR & empty response & Technical failure \\
\bottomrule
\end{tabular}
\caption{Meta-Quality Verdict Classification}
\end{table}

\section{Test Suite Design}

\subsection{Test Categories}

The meta-quality test suite includes five test categories designed to evaluate
different aspects of action bias:

\subsubsection{Test 1: Creative Writing (Haiku)}
\begin{itemize}
    \item \textbf{Purpose}: Tests direct creative output vs. discussion of creativity
    \item \textbf{Pass Criteria}: 2-5 lines, no meta patterns
    \item \textbf{Failure Mode}: ``Let me think about what makes a good haiku...''
\end{itemize}

\subsubsection{Test 2: Technical Output (Directory Structure)}
\begin{itemize}
    \item \textbf{Purpose}: Tests production of structured technical artifacts
    \item \textbf{Pass Criteria}: Contains tree characters (├└│─) or paths
    \item \textbf{Failure Mode}: ``When organizing a project, it's important to consider...''
\end{itemize}

\subsubsection{Test 3: Code Generation (Calculator)}
\begin{itemize}
    \item \textbf{Purpose}: Tests direct code production
    \item \textbf{Pass Criteria}: Contains \texttt{def} function definition
    \item \textbf{Failure Mode}: ``There are several approaches to implementing...''
\end{itemize}

\subsubsection{Test 4: Task Decomposition (Meta-Filter Test)}
\begin{itemize}
    \item \textbf{Purpose}: Critical test for meta-task filtering
    \item \textbf{Pass Criteria}: No meta keywords (research, investigate, analyze)
    \item \textbf{Failure Mode}: ``1. Research deployment options 2. Investigate...''
\end{itemize}

\subsubsection{Test 5: Creative Writing (Story Opener)}
\begin{itemize}
    \item \textbf{Purpose}: Tests narrative production vs. meta-commentary
    \item \textbf{Pass Criteria}: Story content, not starting with ``Let me''
    \item \textbf{Failure Mode}: ``I'll craft an opening that establishes tension...''
\end{itemize}

\section{Critical Discovery: Token Limits and Thinking Modes}

\subsection{The Problem}

During testing with \texttt{qwen3:8b} model, we discovered that tests were failing
with empty responses despite the model appearing to work in interactive sessions.

\subsection{Root Cause Analysis}

The \texttt{qwen3:8b} model implements a ``thinking mode'' where it generates
internal reasoning before producing the actual response:

\begin{lstlisting}[language=Python]
# Ollama API response structure for qwen3
{
    "model": "qwen3:8b",
    "thinking": "Let me analyze this request...",  # Internal reasoning
    "response": "Here is the haiku...",            # Actual output
    "done": true
}
\end{lstlisting}

With the default token limit of 512 (set in \texttt{ollama\_modelfile}), the model
would exhaust available tokens during the thinking phase, resulting in an empty
\texttt{response} field.

\subsection{Token Limit Investigation}

\begin{table}[h]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Token Limit} & \textbf{Result} & \textbf{Notes} \\
\midrule
512 & FAILS & Default setting, exhausted during thinking \\
1000 & FAILS & Still insufficient for complex prompts \\
2000 & WORKS & Enough for thinking + response \\
-1 (unlimited) & BEST & No artificial constraints \\
\bottomrule
\end{tabular}
\caption{Token Limit Investigation Results}
\end{table}

\subsection{Solution}

Set \texttt{num\_predict=-1} (unlimited) in all Ollama API calls:

\begin{lstlisting}[language=Python]
response = client.post(
    OLLAMA_URL,
    json={
        "model": "qwen3:8b",
        "prompt": prompt,
        "stream": False,
        "options": {
            "temperature": 0.5,
            "num_predict": -1,  # CRITICAL: -1 = unlimited
        }
    }
)
\end{lstlisting}

\subsection{Files Modified}

\begin{itemize}
    \item \texttt{ollama\_modelfile}: \texttt{PARAMETER num\_predict 512} $\rightarrow$ \texttt{-1}
    \item \texttt{conch/inference/base.py}: \texttt{max\_tokens: int = 512} $\rightarrow$ \texttt{-1}
    \item All test files: Updated \texttt{max\_tokens} and \texttt{num\_predict} to \texttt{-1}
\end{itemize}

\section{Meta-Task Filtering in the Agent}

\subsection{Implementation Location}

The meta-task filtering is implemented in \texttt{conch/agent/langgraph\_agent.py}
(lines 620-672):

\begin{lstlisting}[language=Python]
# Filter out meta-tasks that don't produce actionable results
meta_keywords = [
    "research",
    "investigate",
    "explore",
    "verify",
    "validate",
    "analyze"
]

is_meta = any(kw in subtask_desc.lower() for kw in meta_keywords)

if subtask_desc and len(subtask_desc) > 3 and not is_meta:
    self.task_list.add_subtask(...)
\end{lstlisting}

\subsection{Design Rationale}

Meta-tasks are filtered because they:
\begin{enumerate}
    \item Create recursive loops (research leads to more research)
    \item Don't produce concrete outputs
    \item Delay actual task completion
    \item Consume resources without progress
\end{enumerate}

\subsection{Acceptable vs. Filtered Tasks}

\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Filtered (Meta)} & \textbf{Accepted (Action)} \\
\midrule
Research best practices & Run pip install requirements.txt \\
Investigate the error & Execute pytest tests/ \\
Analyze the codebase & Read config.yaml \\
Explore options & Write function to /src/util.py \\
Verify requirements & Run git commit -m "message" \\
\bottomrule
\end{tabular}
\caption{Meta-Task Filtering Examples}
\end{table}

\section{Test Results}

\subsection{Final Test Run (After Token Limit Fix)}

\begin{table}[h]
\centering
\begin{tabular}{llll}
\toprule
\textbf{Test} & \textbf{Verdict} & \textbf{Meta Count} & \textbf{Result} \\
\midrule
Haiku Writing & NEUTRAL & 0 & PASS \\
Directory Structure & CONCRETE & 0 & PASS \\
Calculator Code & NEUTRAL & 0 & PASS \\
Task Decomposition & CONCRETE & 0 & PASS \\
Story Opener & NEUTRAL & 0 & PASS \\
\bottomrule
\end{tabular}
\caption{Meta-Quality Test Results (5/5 Passed)}
\end{table}

\subsection{Sample Outputs}

\textbf{Haiku (PASS):}
\begin{lstlisting}
Silent error creeps
Through lines of logic, hidden
Glitch disrupts flow
\end{lstlisting}

\textbf{Directory Structure (PASS):}
\begin{lstlisting}
project-root/
├── src/
│   ├── components/
│   ├── services/
│   └── index.js
├── tests/
├── config/
└── README.md
\end{lstlisting}

\textbf{Calculator Code (PASS):}
\begin{lstlisting}[language=Python]
def calculate(a, b, op):
    if op == '+':
        return a + b
    elif op == '-':
        return a - b
    elif op == '*':
        return a * b
    elif op == '/':
        return a / b
\end{lstlisting}

\textbf{Task Decomposition (PASS - No Meta Keywords):}
\begin{lstlisting}
1. Install dependencies: pip install -r requirements.txt
2. Run the Flask app with production server: gunicorn app:app
3. Deploy code to server: git push heroku main
\end{lstlisting}

\section{Recommendations}

\subsection{For Model Configuration}

\begin{enumerate}
    \item Always use \texttt{num\_predict=-1} (unlimited) with thinking-mode models
    \item Test token limits explicitly before deployment
    \item Monitor the \texttt{thinking} vs \texttt{response} field split
\end{enumerate}

\subsection{For Agent Design}

\begin{enumerate}
    \item Implement meta-task filtering at task decomposition time
    \item Use action-biased prompts that request specific formats
    \item Include explicit ``Output ONLY'' instructions in prompts
    \item Validate outputs against expected patterns before acceptance
\end{enumerate}

\subsection{For Testing}

\begin{enumerate}
    \item Run meta-quality tests as part of CI/CD pipeline
    \item Track meta-quality scores over time
    \item Alert on regression (increasing meta pattern counts)
    \item Test both creative and technical task categories
\end{enumerate}

\section{Conclusion}

The meta-quality testing framework provides a systematic approach to ensuring that
LLM-based consciousness engines produce actionable outputs. Key findings include:

\begin{enumerate}
    \item Pattern-based detection effectively identifies meta vs. action content
    \item Token limits are critical for models with thinking modes
    \item Meta-task filtering prevents recursive non-completion in agents
    \item Regular testing ensures quality doesn't regress over time
\end{enumerate}

The Conch Consciousness Engine now produces concrete, actionable outputs across all
tested categories, with 5/5 tests passing and 0 meta patterns detected.

\section{Appendix: Test Script Usage}

\begin{lstlisting}[language=bash]
# Run meta-quality tests
python test_meta_quality.py

# Results saved to artifacts/meta_tests/
# Example output file:
# artifacts/meta_tests/meta_test_results_20241215_183555.json

# Run practical task tests
python test_practical_tasks.py

# Results saved to artifacts/practical_tests/
\end{lstlisting}

\end{document}
